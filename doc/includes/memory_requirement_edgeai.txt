.. note::
   Memory footprint depends on the chosen model, library configuration and
   whether a full libc (for example: Newlib) is enabled. The numbers shown
   in the repository are examples and will vary between platforms and model
   sizes.

Example guidance
----------------

* Small quantized models (int8) typically have the lowest RAM and flash
  footprint and are suited for constrained devices.
* Larger models or those using floating-point inference will increase
  RAM usage and may require enabling the FPU and a fuller libc implementation.

For exact numbers, build the specific sample and consult the generated
build artifacts (for example the Twister footprint or map file) to derive
precise flash/RAM usage.
